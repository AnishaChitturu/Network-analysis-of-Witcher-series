{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a629ad9-0075-4d0c-90f5-784cd4e6c264",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70b0a24c-2efc-4bb4-bb91-a5a2094986cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99072abd-ea10-4fce-8139-4b39f7d02035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.1-cp311-cp311-macosx_10_12_x86_64.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.0.7-cp311-cp311-macosx_10_9_x86_64.whl (244 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting kiwisolver>=1.0.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.4-cp311-cp311-macosx_10_9_x86_64.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /Users/anishachitturu/anaconda3/envs/witcher_project/lib/python3.11/site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anishachitturu/anaconda3/envs/witcher_project/lib/python3.11/site-packages (from matplotlib) (23.1)\n",
      "Collecting pillow>=6.2.0 (from matplotlib)\n",
      "  Downloading Pillow-9.5.0-cp311-cp311-macosx_10_10_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Downloading pyparsing-3.0.9-py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /Users/anishachitturu/anaconda3/envs/witcher_project/lib/python3.11/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/anishachitturu/anaconda3/envs/witcher_project/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.0.7 cycler-0.11.0 fonttools-4.39.4 kiwisolver-1.4.4 matplotlib-3.7.1 pillow-9.5.0 pyparsing-3.0.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46398b08-2069-4d73-8859-2c7336bbcf8b",
   "metadata": {},
   "source": [
    "#### This whole project is about extracting the relationships between the characters in the witcher book series using NLP. After that, network analysis is applied to transform 1000's of pages into a interactive network graph, to help us better understand the dynamics between the characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f86f9a-5312-4c36-9e8f-46a77d452d10",
   "metadata": {},
   "source": [
    "#### In the witcherwiki website, there is a list of all the books and the each book page contains the list of characters that appear in the book.Our task is to extract all the character names in the books using selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1f047e-e756-4b10-b344-df1601f9acfd",
   "metadata": {},
   "source": [
    "### <font color = blue> Why selenium instead of BeautifulSoup or Scrapy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e66f0f4-539d-47a6-8972-8ca9d97aa83d",
   "metadata": {},
   "source": [
    "### <font color = teal> Selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb4a32a-8350-4500-aed6-6fd3a11f64b3",
   "metadata": {},
   "source": [
    "#### All of them individually can be used for web scrapping purposes or together which would optimise the process. \n",
    "#### Selenium is not actually a web scraping library, it is a browser automation library. It is used to auutomate tasks that you may do as humans on a website like\n",
    "#### opening a browser, moving a mouse, clicking a button etc., So selenium is great for scrapping dynamic web pages that use javascript to store content. Examples websites which have \"See More\" or \"Next\" buttons,  if have to scroll down to see more data , input some data into a form to be able to see more data.\n",
    "#### Selenium can interact with all of these elements and help to get the data out.\n",
    "#### Notice that the URL might not change upon those actions, so other libraries that only get data through URL's can struggle in this case.\n",
    "#### The downsides of Selenium is the datasize it can handle is more limited than Scrapy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab75a3-bf83-4da3-b412-9a0382ce1ead",
   "metadata": {},
   "source": [
    "### <font color = teal > Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3464e925-667d-4fdf-b71c-5f26ee3479cd",
   "metadata": {},
   "source": [
    "#### Scrapy is not even a library, it is a complete web scrapping framework. So it can be used to create a complete spider that can crawl through the entire website in a systematic way. It is fast, powerful and can handle large data. For serious and big web scrapping projects it is better to use Scrapy. To handle dynamic web pages, we can insert Selenium in the parse of the Scrapy spider to automate the clicking, scrolling etc., so that we have best of both worlds.\n",
    "\n",
    "#### However, Scrapy is not beginner friendly and is more suited for advanced users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499b312a-b722-4b80-8fb6-f8f05f4a6175",
   "metadata": {},
   "source": [
    "### <font color = teal > BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda7baee-9be4-49c7-9be9-a7ce89ae97b1",
   "metadata": {},
   "source": [
    "#### BeautifulSoup on the other hand, is a parsing library. There are two steps for scrapping data, first is getting the data from the website and then to parse it and save the output. BeautifulSoup only does the second bit, to actually pull the data from a website, you will need \"requests\" or other libraries.\n",
    "\n",
    "#### BeautifulSoup can automatically detect the structures in the html and xml documents and find what you need. It is beginner friendly and a perfect library for small projects.\n",
    "\n",
    "#### The downsides are that it is a bit slow, has few dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2473cb6-8631-4bb8-b7c2-cb6797522c28",
   "metadata": {},
   "source": [
    "### <font color = teal > Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f06561d-ca26-45d7-974e-ffa1e17cef4c",
   "metadata": {},
   "source": [
    "#### This web scrapping project is quite small and simple, so I chose Selenium."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bdd0be-2a7c-4bde-8341-bd60a29fd8fa",
   "metadata": {},
   "source": [
    "### Create Driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9bb8a6c-c98c-4c8b-b0ad-b8bb18c326b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/bz/9s4mdqhj1ys8_bmyym5162tw0000gn/T/ipykernel_94985/1503906442.py:1: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(ChromeDriverManager().install())\n"
     ]
    }
   ],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "898bb909-dd2b-424d-b49e-8146a4a46cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_url = \"https://witcher.fandom.com/wiki/Category:Characters_in_the_stories\"\n",
    "driver.get(page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9345ddf4-03e3-46f7-affa-f81fac4f5d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find books\n",
    "book_categories = driver.find_elements(by=By.CLASS_NAME, value='category-page__member-link')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0133cd13-b689-47d2-bdc9-71ced593969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = []\n",
    "for category in book_categories:\n",
    "    book_url = category.get_attribute('href')\n",
    "    book_name = category.text\n",
    "    books.append({'book_name': book_name, \"url\": book_url})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1c636c9-0292-4b9e-ad1b-195cb4d55f68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'book_name': 'Category:Baptism of Fire characters',\n",
       "  'url': 'https://witcher.fandom.com/wiki/Category:Baptism_of_Fire_characters'},\n",
       " {'book_name': 'Category:Blood of Elves characters',\n",
       "  'url': 'https://witcher.fandom.com/wiki/Category:Blood_of_Elves_characters'},\n",
       " {'book_name': \"Godamba Thaess'en\",\n",
       "  'url': 'https://witcher.fandom.com/wiki/Godamba_Thaess%27en'},\n",
       " {'book_name': 'Category:Season of Storms characters',\n",
       "  'url': 'https://witcher.fandom.com/wiki/Category:Season_of_Storms_characters'},\n",
       " {'book_name': 'Category:Something Ends, Something Begins characters',\n",
       "  'url': 'https://witcher.fandom.com/wiki/Category:Something_Ends,_Something_Begins_characters'},\n",
       " {'book_name': 'Category:Sword of Destiny characters',\n",
       "  'url': 'https://witcher.fandom.com/wiki/Category:Sword_of_Destiny_characters'},\n",
       " {'book_name': 'Category:Szpony i kły characters',\n",
       "  'url': 'https://witcher.fandom.com/wiki/Category:Szpony_i_k%C5%82y_characters'},\n",
       " {'book_name': 'Category:Tales from the world of The Witcher characters',\n",
       "  'url': 'https://witcher.fandom.com/wiki/Category:Tales_from_the_world_of_The_Witcher_characters'},\n",
       " {'book_name': 'Category:The Lady of the Lake characters',\n",
       "  'url': 'https://witcher.fandom.com/wiki/Category:The_Lady_of_the_Lake_characters'},\n",
       " {'book_name': 'Category:The Last Wish characters',\n",
       "  'url': 'https://witcher.fandom.com/wiki/Category:The_Last_Wish_characters'},\n",
       " {'book_name': 'Category:The Tower of the Swallow characters',\n",
       "  'url': 'https://witcher.fandom.com/wiki/Category:The_Tower_of_the_Swallow_characters'},\n",
       " {'book_name': 'Category:Time of Contempt characters',\n",
       "  'url': 'https://witcher.fandom.com/wiki/Category:Time_of_Contempt_characters'}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbc1c4e-4deb-49c2-adbf-f3f7f1336479",
   "metadata": {},
   "outputs": [],
   "source": [
    "character_list = []\n",
    "\n",
    "for book in books:\n",
    "    # go to book page\n",
    "    driver.get(book['url'])\n",
    "    \n",
    "    character_elems = driver.find_elements(by=By.CLASS_NAME, value = 'category-page__member-link')\n",
    "    \n",
    "    for elem in character_elems:\n",
    "        character_list.append({'book': book['book_name'],'character': elem.text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf324fb-0124-4f26-8421-d60ede3c1bde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
